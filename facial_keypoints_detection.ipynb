{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "idLookUpTable = pd.read_csv(\"idLookUpTable.csv\")\n",
    "train_set = pd.read_csv(\"training.csv\")\n",
    "test_set = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,image in enumerate(train_set.Image):\n",
    "    train_set[\"Image\"][idx] = np.fromstring(image, dtype=int, sep=\" \").reshape(96,96)\n",
    "\n",
    "    \n",
    "for idx,image in enumerate(test_set.Image):\n",
    "    test_set[\"Image\"][idx] = np.fromstring(image, dtype=int, sep=\" \").reshape(96,96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_set.iloc[0].Image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.dropna(subset=[\"left_eye_center_x\",\"right_eye_center_x\",\"mouth_center_bottom_lip_x\"], inplace=True)\n",
    "#train_set.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing value imputation\n",
    "\n",
    "train_set_notNull = train_set.dropna(axis=0, inplace=False)\n",
    "\n",
    "left_eye_inner_corner_x_diff = (train_set_notNull.left_eye_inner_corner_x - train_set_notNull.left_eye_center_x).mean()\n",
    "left_eye_inner_corner_y_diff = (train_set_notNull.left_eye_inner_corner_y - train_set_notNull.left_eye_center_y).mean()\n",
    "right_eye_inner_corner_x_diff = (train_set_notNull.right_eye_inner_corner_x - train_set_notNull.right_eye_center_x).mean()\n",
    "right_eye_inner_corner_y_diff = (train_set_notNull.right_eye_inner_corner_y - train_set_notNull.right_eye_center_y).mean()\n",
    "\n",
    "left_eye_outer_corner_x_diff = (train_set_notNull.left_eye_outer_corner_x - train_set_notNull.left_eye_center_x).mean()\n",
    "left_eye_outer_corner_y_diff = (train_set_notNull.left_eye_outer_corner_y - train_set_notNull.left_eye_center_y).mean()\n",
    "right_eye_outer_corner_x_diff = (train_set_notNull.right_eye_outer_corner_x - train_set_notNull.right_eye_center_x).mean()\n",
    "right_eye_outer_corner_y_diff = (train_set_notNull.right_eye_outer_corner_y - train_set_notNull.right_eye_center_y).mean()\n",
    "\n",
    "left_eyebrow_inner_end_x_diff = (train_set_notNull.left_eyebrow_inner_end_x - train_set_notNull.left_eye_center_x).mean()\n",
    "left_eyebrow_inner_end_y_diff = (train_set_notNull.left_eyebrow_inner_end_y - train_set_notNull.left_eye_center_y).mean()\n",
    "right_eyebrow_inner_end_x_diff = (train_set_notNull.right_eyebrow_inner_end_x - train_set_notNull.right_eye_center_x).mean()\n",
    "right_eyebrow_inner_end_y_diff = (train_set_notNull.right_eyebrow_inner_end_y - train_set_notNull.right_eye_center_y).mean()\n",
    "\n",
    "left_eyebrow_outer_end_x_diff = (train_set_notNull.left_eyebrow_outer_end_x - train_set_notNull.left_eye_center_x).mean()\n",
    "left_eyebrow_outer_end_y_diff = (train_set_notNull.left_eyebrow_outer_end_y - train_set_notNull.left_eye_center_y).mean()\n",
    "right_eyebrow_outer_end_x_diff = (train_set_notNull.right_eyebrow_outer_end_x - train_set_notNull.right_eye_center_x).mean()\n",
    "right_eyebrow_outer_end_y_diff = (train_set_notNull.right_eyebrow_outer_end_y - train_set_notNull.right_eye_center_y).mean()\n",
    "\n",
    "mouth_left_corner_x_diff = (train_set_notNull.mouth_left_corner_x - train_set_notNull.mouth_center_bottom_lip_x).mean()\n",
    "mouth_left_corner_y_diff = (train_set_notNull.mouth_left_corner_y - train_set_notNull.mouth_center_bottom_lip_y).mean()\n",
    "\n",
    "mouth_right_corner_x_diff = (train_set_notNull.mouth_right_corner_x - train_set_notNull.mouth_center_bottom_lip_x).mean()\n",
    "mouth_right_corner_y_diff = (train_set_notNull.mouth_right_corner_y - train_set_notNull.mouth_center_bottom_lip_y).mean()\n",
    "\n",
    "mouth_center_top_lip_x_diff = (train_set_notNull.mouth_center_top_lip_x - train_set_notNull.mouth_center_bottom_lip_x).mean()\n",
    "mouth_center_top_lip_y_diff = (train_set_notNull.mouth_center_top_lip_y - train_set_notNull.mouth_center_bottom_lip_y).mean() + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"left_eye_inner_corner_x\"].fillna(train_set.left_eye_center_x + left_eye_inner_corner_x_diff, inplace=True)\n",
    "train_set[\"left_eye_inner_corner_y\"].fillna(train_set.left_eye_center_y + left_eye_inner_corner_y_diff, inplace=True)\n",
    "train_set[\"left_eye_outer_corner_x\"].fillna(train_set.left_eye_center_x + left_eye_outer_corner_x_diff, inplace=True)\n",
    "train_set[\"left_eye_outer_corner_y\"].fillna(train_set.left_eye_center_y + left_eye_outer_corner_y_diff, inplace=True)\n",
    "\n",
    "train_set[\"right_eye_inner_corner_x\"].fillna(train_set.right_eye_center_x + right_eye_inner_corner_x_diff, inplace=True)\n",
    "train_set[\"right_eye_inner_corner_y\"].fillna(train_set.right_eye_center_y + right_eye_inner_corner_y_diff, inplace=True)\n",
    "train_set[\"right_eye_outer_corner_x\"].fillna(train_set.right_eye_center_x + right_eye_outer_corner_x_diff, inplace=True)\n",
    "train_set[\"right_eye_outer_corner_y\"].fillna(train_set.right_eye_center_y + right_eye_outer_corner_y_diff, inplace=True)\n",
    "\n",
    "train_set[\"left_eyebrow_inner_end_x\"].fillna(train_set.left_eye_center_x + left_eyebrow_inner_end_x_diff, inplace=True)\n",
    "train_set[\"left_eyebrow_inner_end_y\"].fillna(train_set.left_eye_center_y + left_eyebrow_inner_end_y_diff, inplace=True)\n",
    "train_set[\"left_eyebrow_outer_end_x\"].fillna(train_set.left_eye_center_x + left_eyebrow_outer_end_x_diff, inplace=True)\n",
    "train_set[\"left_eyebrow_outer_end_y\"].fillna(train_set.left_eye_center_y + left_eyebrow_outer_end_y_diff, inplace=True)\n",
    "\n",
    "train_set[\"right_eyebrow_inner_end_x\"].fillna(train_set.right_eye_center_x + right_eyebrow_inner_end_x_diff, inplace=True)\n",
    "train_set[\"right_eyebrow_inner_end_y\"].fillna(train_set.right_eye_center_y + right_eyebrow_inner_end_y_diff, inplace=True)\n",
    "train_set[\"right_eyebrow_outer_end_x\"].fillna(train_set.right_eye_center_x + right_eyebrow_outer_end_x_diff, inplace=True)\n",
    "train_set[\"right_eyebrow_outer_end_y\"].fillna(train_set.right_eye_center_y + right_eyebrow_outer_end_y_diff, inplace=True)\n",
    "\n",
    "train_set[\"mouth_left_corner_x\"].fillna(train_set.mouth_center_bottom_lip_x + mouth_left_corner_x_diff, inplace=True)\n",
    "train_set[\"mouth_left_corner_y\"].fillna(train_set.mouth_center_bottom_lip_y + mouth_left_corner_y_diff, inplace=True)\n",
    "\n",
    "train_set[\"mouth_right_corner_x\"].fillna(train_set.mouth_center_bottom_lip_x + mouth_right_corner_x_diff, inplace=True)\n",
    "train_set[\"mouth_right_corner_y\"].fillna(train_set.mouth_center_bottom_lip_y + mouth_right_corner_y_diff, inplace=True)\n",
    "\n",
    "train_set[\"mouth_center_top_lip_x\"].fillna(train_set.mouth_center_bottom_lip_x + mouth_center_top_lip_x_diff, inplace=True)\n",
    "train_set[\"mouth_center_top_lip_y\"].fillna(train_set.mouth_center_bottom_lip_y + mouth_center_top_lip_y_diff, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set, val_set = np.split(train_set,[int(.9*len(train_set))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotKeypoints(data):\n",
    "    plt.imshow(data.Image, cmap=\"gray\")\n",
    "    plt.scatter(data[:-1:2], data[1:-1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotKeypoints(train_set.iloc[6943])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['keypoints']\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'keypoints': torch.tensor(landmarks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "class faceLandmarksDataset(Dataset):\n",
    "    def __init__(self, data ,transform=ToTensor()):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        keypoints = self.data.iloc[idx,:-1]\n",
    "        image = self.data.iloc[idx].Image\n",
    "        \n",
    "        sample = {'image': image, 'keypoints': keypoints}\n",
    "        \n",
    "        return self.transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceKeypointDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceKeypointDetectionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32,64, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(64,96, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(96,128, kernel_size=3)\n",
    "        self.conv5 = nn.Conv2d(128,256, kernel_size=3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(9216, 1024)\n",
    "        self.fc2 = nn.Linear(1024,128)\n",
    "        self.fc3 = nn.Linear(128, 30)\n",
    "        \n",
    "        self.BN1 = nn.BatchNorm2d(32)\n",
    "        self.BN2 = nn.BatchNorm2d(64)\n",
    "        self.BN3 = nn.BatchNorm2d(96)\n",
    "        self.BN4 = nn.BatchNorm2d(128)\n",
    "        self.BN5 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.pool = nn.AvgPool2d(2,2)\n",
    "        #self.pool = nn.MaxPool2d(2,2)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.BN1(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.BN2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x)) \n",
    "        x = self.BN3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x)) \n",
    "        x = self.BN4(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.conv5(x)) \n",
    "        x = self.BN5(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        \n",
    "        x = flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, epochs, optimizer, criterion, train_loader, val_loader = None):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        network.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            images, keyPoints = data[\"image\"].float().to(device) , data[\"keypoints\"].float().to(device)\n",
    "            images = images.unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            output = network(images)\n",
    "            loss = criterion(output, keyPoints)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss /= len(train_loader)\n",
    "        print('Train Epoch: {} \\tTrain Loss: {:.6f}'.format(epoch, epoch_loss))\n",
    "\n",
    "        if ( val_loader is not None ):\n",
    "            network.eval()\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for batch_idx, data in enumerate(val_loader):\n",
    "                images, keyPoints = data[\"image\"].float().to(device) , data[\"keypoints\"].float().to(device)\n",
    "                images = images.unsqueeze(1)\n",
    "                optimizer.zero_grad()\n",
    "                output = network(images)\n",
    "                loss = criterion(output, keyPoints)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            epoch_loss /= len(val_loader)\n",
    "            print('Train Epoch: {} \\tValidation Loss: {:.6f}'.format(epoch, epoch_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = faceLandmarksDataset(train_set)\n",
    "#valDataset = faceLandmarksDataset(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(trainDataset, batch_size = 16)\n",
    "#valLoader = DataLoader(valDataset, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceKeypointDetectionModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-convergence",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(model.to(device), 1, optimizer, criterion, trainLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), <pre_trained_model_path>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_trained_model_path = \"/content/drive/MyDrive/facial_keypoint_detection/model_final\")\n",
    "#model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_id = 716\n",
    "\n",
    "test_image = torch.tensor(test_set.iloc[test_image_id].Image)\n",
    "plt.imshow(test_image, cmap=\"gray\")\n",
    "pred = model(torch.tensor(test_image).unsqueeze(0).unsqueeze(1).float().to(device))\n",
    "pred = torch.tensor(pred.squeeze().to(device))\n",
    "plt.scatter(pred[::2].to(\"cpu\"), pred[1::2].to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = torch.tensor(test_set.Image).reshape(-1,1,96,96).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(test_images[0].reshape(-1,1,96,96))\n",
    "    for image in test_images[1:]:\n",
    "        pred = torch.cat((pred,model(image.reshape(-1,1,96,96))),0)\n",
    "    \n",
    "    pred = abs(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookid_list = list(lookid_data[\"FeatureName\"])\n",
    "imageID = list(lookid_data[\"ImageId\"]-1)\n",
    "pred_list = list(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowid = lookid_data['RowId']\n",
    "rowid=list(rowid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = []\n",
    "for f in list(lookid_data['FeatureName']):\n",
    "    feature.append(lookid_list.index(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "preded = []\n",
    "for x,y in zip(imageID,feature):\n",
    "    preded.append(float(pred_list[x][y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowid = pd.Series(rowid,name = 'RowId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = pd.Series(preded,name = 'Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([rowid,loc],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('/content/drive/MyDrive/facial_keypoint_detection/face_key_detection_submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_id = 122\n",
    "\n",
    "plt.imshow(test_set.iloc[test_image_id].Image, cmap=\"gray\")\n",
    "plt.scatter(pred[test_image_id][::2].to(\"cpu\"), pred[test_image_id][1::2].to(\"cpu\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
